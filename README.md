# ChatGPT-based Test Generation for Refactoring Engines Enhanced by  Feature Analysis on Examples
Software refactoring is widely employed to improve software quality. However, conducting refactorings manually is tedious, time-consuming, and error-prone. Consequently, automated and semi-automated tool support is highly desirable for software refactoring in the industry, and most of the main-stream IDEs provide powerful support for refactoring. However, complex refactoring engines are prone to errors, which in turn may result in imperfect and incorrect refactorings. To this end, in this paper, we propose a ChatGPT-based approach to testing refactoring engines. We first manually analyze bug reports and test cases associated with refactoring engines, and construct a feature library containing fine-grained features that may trigger defects in refactoring engines. The approach automatically generates prompts according to both predefined prompt templates and features randomly selected from the feature library, requesting ChatGPT to generate test programs with the requested features. Test programs generated by ChatGPT are then forwarded to multiple refactoring engines for differential testing. To the best of our knowledge, it is the first approach in this line that guides test program generation with features derived from existing bugs. It is also the first approach in this line that exploits LLMs in the generation of test programs. Our initial evaluation on four main-stream refactoring engines suggests that the proposed approach is effective. It identified a total of 115 previously unknown bugs besides 28 inconsistent refactoring behaviors among different engines. Among the 115 bugs, 78 have been manually confirmed by the original developers of the tested engines, i.e.,  IntelliJ IDEA, Eclipse, VScode-Java, and NetBeans.  

We provide a [Bug Repository](https://assdfsdafasfa.github.io/) that contains all details (test case code before and after refactoring, error reasons, expected results). 

Auto-perform refactoring tools in [RefEclipse](https://github.com/assdfsdafasfa/OpenPaper/tree/main/Eclipse_AutoRefactor) and [RefIDEA](https://github.com/assdfsdafasfa/OpenPaper/tree/main/IDEA_AutoRefactor) package.
Analysis of historical bug in [AnalysisOfHistoricalBug](https://github.com/assdfsdafasfa/OpenPaper/tree/main/AnalysisOfHistoricalBug) package, document nameï¼š**BugAnalysisList.xls**

# BaseLine
**Paper: Systematic Testing of Refactoring Engines on Real Software Projects**  

**[BaeLine URL](http://mir.cs.illinois.edu/rtr)** 

# Results
The case study should answer the following research questions:

RQ1 Can the proposed approach identify unknown defects in widely-used and extensively-tested refactoring engines?

RQ2 To what extent can the proposed approach improve the state of the art?

RQ3 Can the proposed approach be applied to test additional refactoring engines without additional human analysis/adaption? 

**Result:**

![image](https://github.com/user-attachments/assets/602e2bda-17c9-4a4d-b87a-4d6c40ea0e6d)    ![image](https://github.com/user-attachments/assets/912a5986-bb31-4b31-ab99-33b3c7443a08)


**Compare SOTA:**

![image](https://github.com/user-attachments/assets/795ee238-3245-4941-985c-e592b1d2b4a9)


**Additional Refactoring Engines:**

![image](https://github.com/user-attachments/assets/cf638153-fe73-46c8-8897-7401f5317df9)
